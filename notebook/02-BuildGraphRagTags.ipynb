{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook for Graph Building\n",
    "\n",
    "This notebook demonstrates how to use the `process_graph` function from `build_graph.py` to process JSON files and build/prune a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599c4c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/eak/learning/nganga_ai/graphrag-tagger\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graph...\n",
      "Found 81 files in notebook/example/results.\n",
      "Filtering by content type: paragraph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading raw files: 100%|██████████| 81/81 [00:00<00:00, 15172.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 58 raw documents.\n",
      "Computing scores...\n",
      "Scores computed.\n",
      "Building graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building nodes & edges: 100%|██████████| 58/58 [00:00<00:00, 11417.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built. Nodes: 58 Edges: 990\n",
      "Starting graph pruning...\n",
      "Min weight: 2.2142291753826795\n",
      "Max weight: 11.656063389192791\n",
      "Mean weight: 4.024045879185529\n",
      "Median weight: 3.547562508716013\n",
      "Pruning threshold (97.5th percentile): 7.651970479746955\n",
      "Removing 963 edges out of 990...\n",
      "Graph pruned. Nodes: 58 Edges: 27\n",
      "Computing connected components...\n",
      "Number of connected components: 43\n",
      "Component sizes (min, max, mean): 1 8 1.3488372093023255\n",
      "Connected components map saved to notebook/example/results/graph_outputs/connected_components.json\n",
      "Graph processing complete.\n",
      "Graph processing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from graphrag_tagger.build_graph import process_graph\n",
    "\n",
    "# Define sample input and output folders\n",
    "input_folder = (\n",
    "    \"notebook/example/results\"  # update this path to your folder containing JSON files\n",
    ")\n",
    "output_folder = \"notebook/example/results/graph_outputs\"  # update this path to where you want the results saved\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process graph with a specified threshold percentile (e.g., 97.5)\n",
    "graph = process_graph(\n",
    "    input_folder,\n",
    "    output_folder,\n",
    "    threshold_percentile=97.5,\n",
    "    content_type_filter=\"paragraph\", # Use only paragraph for chunk graph builder\n",
    ")\n",
    "\n",
    "# The processed graph is stored in 'graph' and the connected components map is saved to the output folder.\n",
    "print(\"Graph processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a95bd824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "raw: dict = json.load(open(os.path.join(output_folder, \"connected_components.json\")))\n",
    "\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3953f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0bbbd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(raw.values())) # unique tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c092959c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connected_chunks = {}\n",
    "for k, v in raw.items():\n",
    "    if v in connected_chunks:\n",
    "        connected_chunks[v].append(int(k) + 1)\n",
    "    else:\n",
    "        connected_chunks[v] = [int(k) + 1]\n",
    "        \n",
    "len(connected_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e3f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [1, 3, 13, 21, 56, 26, 27, 30]\n",
      "8 [41, 10, 42, 46, 47]\n",
      "10 [12, 29]\n",
      "12 [15, 23]\n",
      "27 [36, 37]\n",
      "31 [43, 52]\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "\n",
    "for k, v in connected_chunks.items():\n",
    "    if len(v) > 1:\n",
    "        print(k, v)\n",
    "        if len(v) > len(examples):\n",
    "            examples = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14528b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = json.load(open(os.path.join(input_folder, f\"chunk_{examples[0]}.json\")))\n",
    "example2 = json.load(open(os.path.join(input_folder, f\"chunk_{examples[1]}.json\")))\n",
    "example3 = json.load(open(os.path.join(input_folder, f\"chunk_{examples[2]}.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8790414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
      "QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional\n",
      "RAG baseline for both the comprehensiveness and diversity of generated answers.\n",
      "1\n",
      "Introduction\n",
      "Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using\n"
     ]
    }
   ],
   "source": [
    "print(example1[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddcbc85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generates a response based on both the query and the retrieved records (Baumel et al., 2018; Dang, 2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call vector RAG, works well for queries that can be answered with information localized within a small set of records. However, vector RAG approaches do not support sensemaking queries, meaning queries that require global understanding of the entire dataset, such as ”What are the key trends in how scientific discoveries are influenced by interdisciplinary research over the past decade?”\n",
      "Sensemaking tasks require reasoning over “connections (which can be among people, places, and events) in order to anticipate their trajectories and act effectively” (Klein et al., 2006). LLMs such as GPT (Achiam et al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) excel at sensemaking in complex domains like scientific discovery (Microsoft, 2023)\n"
     ]
    }
   ],
   "source": [
    "print(example2[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a48e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n",
      "Source Documents →Text Chunks\n",
      "To start, the documents in the corpus are split into text chunks. The LLM extracts information from each chunk for downstream processing. Selecting the size of the chunk is a fundamental design decision; longer text chunks require fewer LLM calls for such extraction (which reduces cost) but suffer from degraded recall of information that appears early in the chunk (Kuratov et al., 2024; Liu et al., 2023). See Section A.1 for prompts and examples of the recall-precision trade-offs.\n",
      "3.1.2\n",
      "Text Chunks →Entities & Relationships\n",
      "In this step, the LLM is prompted to extract instances of important entities and the relationships between the entities from a given chunk. Additionally, the LLM generates short descriptions for the entities and relationships. To illustrate, suppose a chunk contained the following text: 4\n"
     ]
    }
   ],
   "source": [
    "print(example3[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "417f6689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'content_type': 'paragraph',\n",
       "  'is_sufficient': True,\n",
       "  'topics': ['Topic 1', 'Topic 4', 'Topic 2']},\n",
       " {'content_type': 'paragraph',\n",
       "  'is_sufficient': True,\n",
       "  'topics': ['Topic 4', 'Topic 8', 'Topic 6']},\n",
       " {'content_type': 'paragraph',\n",
       "  'is_sufficient': True,\n",
       "  'topics': ['Topic 4', 'Topic 5']})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1[\"classification\"], example2[\"classification\"], example3[\"classification\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7feea563",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = \"\\n\\n\".join([\n",
    "    json.load(open(os.path.join(input_folder, f\"chunk_{example}.json\")))[\"chunk\"] for example in sorted(examples)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "886d7366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "focused summarization (QFS) task, rather than an explicit retrieval task. Prior\n",
      "QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional\n",
      "RAG baseline for both the comprehensiveness and diversity of generated answers.\n",
      "1\n",
      "Introduction\n",
      "Retrieval augmented generation (RAG) (Lewis et al., 2020) is an established approach to using\n",
      "\n",
      "generates a response based on both the query and the retrieved records (Baumel et al., 2018; Dang, 2006; Laskar et al., 2020; Yao et al., 2017). This conventional approach, which we collectively call vector RAG, works well for queries that can be answered with information localized within a small set of records. However, vector RAG approaches do not support sensemaking queries, meaning queries that require global understanding of the entire dataset, such as ”What are the key trends in how scientific discoveries are influenced by interdisciplinary research over the past decade?”\n",
      "Sensemaking tasks require reasoning over “connections (which can be among people, places, and events) in order to anticipate their trajectories and act effectively” (Klein et al., 2006). LLMs such as GPT (Achiam et al., 2023; Brown et al., 2020), Llama (Touvron et al., 2023), and Gemini (Anil et al., 2023) excel at sensemaking in complex domains like scientific discovery (Microsoft, 2023)\n",
      "\n",
      "3.1.1\n",
      "Source Documents →Text Chunks\n",
      "To start, the documents in the corpus are split into text chunks. The LLM extracts information from each chunk for downstream processing. Selecting the size of the chunk is a fundamental design decision; longer text chunks require fewer LLM calls for such extraction (which reduces cost) but suffer from degraded recall of information that appears early in the chunk (Kuratov et al., 2024; Liu et al., 2023). See Section A.1 for prompts and examples of the recall-precision trade-offs.\n",
      "3.1.2\n",
      "Text Chunks →Entities & Relationships\n",
      "In this step, the LLM is prompted to extract instances of important entities and the relationships between the entities from a given chunk. Additionally, the LLM generates short descriptions for the entities and relationships. To illustrate, suppose a chunk contained the following text: 4\n",
      "\n",
      "3.2\n",
      "Global Sensemaking Question Generation\n",
      "To evaluate the effectiveness of RAG systems for global sensemaking tasks, we use an LLM to generate a set of corpus-specific questions designed to asses high-level understanding of a given corpus, without requiring retrieval of specific low-level facts. Instead, given a high-level description of a corpus and its purposes, the LLM is prompted to generate personas of hypothetical users of the RAG system. For each hypothetical user, the LLM is then prompted to specify tasks that this user would use the RAG system to complete. Finally, for each combination of user and task, the\n",
      "LLM is prompted to generate questions that require understanding of the entire corpus. Algorithm 1 describes the approach.\n",
      "6\n",
      "\n",
      "Experiment 2\n",
      "To validate the comprehensiveness and diversity results from Experiment 1, we implemented claimbased measures of these qualities. We use the definition of a factual claim from Ni et al. (2024), which is “a statement that explicitly presents some verifiable facts.” For example, the sentence “California and New York implemented incentives for renewable energy adoption, highlighting the broader importance of sustainability in policy decisions” contains two factual claims: (1) California implemented incentives for renewable energy adoption, and (2) New York implemented incentives for renewable energy adoption.\n",
      "To extract factual claims, we used Claimify (Metropolitansky and Larson, 2025), an LLM-based method that identifies sentences in an answer containing at least one factual claim, then decomposes these sentences into simple, self-contained factual claims. We applied Claimify to the answers generated under the conditions from Experiment 1. After removing duplicate claims from each answer, we extracted 47,075 unique claims, with an average of 31 claims per answer.\n",
      "We defined two metrics, with higher values indicating better performance:\n",
      "\n",
      "the number of community summaries at different levels of each graph community hierarchy.\n",
      "Global approaches vs. vector RAG. As shown in Figure 2 and Table 6, global approaches significantly outperformed conventional vector RAG (SS) in both comprehensiveness and diversity criteria across datasets. Specifically, global approaches achieved comprehensiveness win rates between 7283% (p<.001) for Podcast transcripts and 72-80% (p<.001) for News articles, while diversity win rates ranged from 75-82% (p<.001) and 62-71% (p<.01) respectively. Our use of directness as a validity test confirmed that vector RAG produces the most direct responses across all comparisons.\n",
      "Empowerment. Empowerment comparisons showed mixed results for both global approaches versus vector RAG (SS) and GraphRAG approaches versus source text summarization (TS). Using an\n",
      "LLM to analyze LLM reasoning for this measure indicated that the ability to provide specific exam9\n",
      "\n",
      "the LLM aligned with the winner based on the claim-based metrics. Since each pairwise comparison in Experiment 1 was performed five times, while the claim-based metrics provided only one outcome per comparison, we aggregated the Experiment 1 results into a single label using majority voting.\n",
      "For example, if C0 won over SS in three out of five judgments for comprehensiveness on a given question, C0 was labeled the winner and SS the loser. However, if C0 won twice, SS won once, and they tied twice, then there was no majority outcome, so the final label was a tie.\n",
      "We found that exact ties were rare for the claim-based metrics. One possible solution is to define a tie based on a threshold (e.g., the absolute difference between the claim-based results for condition\n",
      "A and condition B must be less than or equal to x). However, we observed that the results were sensitive to the choice of threshold. As a result, we focused on cases where the aggregated LLM label was not a tie, representing 33% and 39% of pairwise comparisons for comprehensiveness and\n",
      "\n",
      "Zhang, Y., Zhang, Y., Gan, Y., Yao, L., and Wang, C. (2024a). Causal graph discovery with retrievalaugmented generation based large language models. arXiv preprint arXiv:2402.15301.\n",
      "Zhang, Z., Chen, J., and Yang, D. (2024b). Darg: Dynamic evaluation of large language models via adaptive reasoning graph. arXiv preprint arXiv:2406.17271.\n",
      "Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing,\n",
      "E., et al. (2024). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural\n",
      "Information Processing Systems, 36.\n",
      "Zhu, Y., Wang, X., Chen, J., Qiao, S., Ou, Y., Yao, Y., Deng, S., Chen, H., and Zhang, N. (2024).\n",
      "Llms for knowledge graph construction and reasoning: Recent capabilities and future opportunities.\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db38ece7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
