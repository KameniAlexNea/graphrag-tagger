{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Notebook for Graph Building\n",
    "\n",
    "This notebook demonstrates how to use the `process_graph` function from `build_graph.py` to process JSON files and build/prune a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599c4c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/eak/learning/nganga_ai/graphrag-tagger\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graph...\n",
      "Found 114 files in notebook/example/results.\n",
      "Filtering by content type: paragraph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading raw files: 100%|██████████| 114/114 [00:00<00:00, 16350.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 90 raw documents.\n",
      "Computing scores...\n",
      "Scores computed.\n",
      "Building graph...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building nodes & edges: 100%|██████████| 90/90 [00:00<00:00, 6990.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built. Nodes: 90 Edges: 2723\n",
      "Starting graph pruning...\n",
      "Min weight: 2.187051524018611\n",
      "Max weight: 8.949035849482943\n",
      "Mean weight: 3.9093317790378714\n",
      "Median weight: 3.353718190685278\n",
      "Pruning threshold (97.5th percentile): 6.928650992130997\n",
      "Removing 2652 edges out of 2723...\n",
      "Graph pruned. Nodes: 90 Edges: 71\n",
      "Computing connected components...\n",
      "Number of connected components: 68\n",
      "Component sizes (min, max, mean): 1 9 1.3235294117647058\n",
      "Connected components map saved to notebook/example/results/graph_outputs/connected_components.json\n",
      "Graph processing complete.\n",
      "Graph processing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from graphrag_tagger.build_graph import process_graph\n",
    "\n",
    "# Define sample input and output folders\n",
    "input_folder = (\n",
    "    \"notebook/example/results\"  # update this path to your folder containing JSON files\n",
    ")\n",
    "output_folder = \"notebook/example/results/graph_outputs\"  # update this path to where you want the results saved\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process graph with a specified threshold percentile (e.g., 97.5)\n",
    "graph = process_graph(\n",
    "    input_folder,\n",
    "    output_folder,\n",
    "    threshold_percentile=97.5,\n",
    "    content_type_filter=\"paragraph\",\n",
    ")\n",
    "\n",
    "# The processed graph is stored in 'graph' and the connected components map is saved to the output folder.\n",
    "print(\"Graph processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a95bd824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "raw: dict = json.load(open(os.path.join(output_folder, \"connected_components.json\")))\n",
    "\n",
    "len(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b3953f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0bbbd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(raw.values())) # unique tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c092959c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connected_chunks = {}\n",
    "for k, v in raw.items():\n",
    "    if v in connected_chunks:\n",
    "        connected_chunks[v].append(int(k) + 1)\n",
    "    else:\n",
    "        connected_chunks[v] = [int(k) + 1]\n",
    "        \n",
    "len(connected_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e3f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [65, 2, 43, 47, 51, 62, 63]\n",
      "14 [41, 15]\n",
      "16 [34, 35, 39, 17, 18, 19, 26, 27, 31]\n",
      "31 [50, 40]\n",
      "32 [57, 42, 59, 58]\n",
      "59 [83, 84, 85, 79]\n"
     ]
    }
   ],
   "source": [
    "for k, v in connected_chunks.items():\n",
    "    if len(v) > 1:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14528b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = json.load(open(os.path.join(input_folder, \"chunk_43.json\")))\n",
    "example2 = json.load(open(os.path.join(input_folder, \"chunk_51.json\")))\n",
    "example3 = json.load(open(os.path.join(input_folder, \"chunk_63.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8790414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to source text summarization: for low-level community summaries (C3), GraphRAG required 26-\n",
      "33% fewer context tokens, while for root-level community summaries (C0), it required over 97%\n",
      "fewer tokens. For a modest drop in performance compared with other global methods, root-level\n",
      "GraphRAG offers a highly efficient method for the iterative question answering that characterizes\n",
      "sensemaking activity, while retaining advantages in comprehensiveness (72% win rate) and diversity\n",
      "(62% win rate) over vector RAG.\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(example1[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddcbc85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acknowledgements\n",
      "We would also like to thank the following people who contributed to the work: Alonso Guevara\n",
      "Fern´andez, Amber Hoak, Andr´es Morales Esquivel, Ben Cutler, Billie Rinaldi, Chris Sanchez,\n",
      "Chris Trevino, Christine Caggiano, David Tittsworth, Dayenne de Souza, Douglas Orbaker, Ed\n",
      "Clark, Gabriel Nieves-Ponce, Gaudy Blanco Meneses, Kate Lytvynets, Katy Smith, M´onica Carva-\n",
      "jal, Nathan Evans, Richard Ortega, Rodrigo Racanicci, Sarah Smith, and Shane Solomon.\n",
      "References\n",
      "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Al-\n",
      "tenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint\n",
      "arXiv:2303.08774.\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(example2[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5a48e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval for open-domain question answering. arXiv preprint arXiv:2009.08553.\n",
      "Martin, S., Brown, W. M., Klavans, R., and Boyack, K. (2011). Openord: An open-source toolbox\n",
      "for large graph layout. SPIE Conference on Visualization and Data Analysis (VDA).\n",
      "Melnyk, I., Dognin, P., and Das, P. (2022). Knowledge graph generation from text.\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(example3[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3d94af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = json.load(open(os.path.join(input_folder, \"chunk_83.json\")))\n",
    "example2 = json.load(open(os.path.join(input_folder, \"chunk_85.json\")))\n",
    "example3 = json.load(open(os.path.join(input_folder, \"chunk_79.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7d97359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and structured overview of public figures across various sectors of the entertainment industry,\n",
      "including film, television, music, sports, and digital media. It lists multiple individuals, providing\n",
      "specific examples of their contributions and the context in which they are mentioned in entertainment\n",
      "articles, along with references to data reports for each claim. This approach helps the reader\n",
      "understand the breadth of the topic and make informed judgments without being misled. In contrast,\n",
      "Answer 2 focuses on a smaller group of public figures and primarily discusses their personal lives and\n",
      "relationships, which may not provide as broad an understanding of the topic. While Answer 2 also\n",
      "cites sources, it does not match the depth and variety of Answer 1.\n",
      "Directness: Winner=2 (Na¨ıve RAG). Answer 2 is better because it directly lists specific public\n",
      "figures who are repeatedly mentioned across various entertainment articles, such as Taylor Swift,\n",
      "Travis Kelce, Britney Spears, and Justin Timberlake, and provides concise explanations for their\n",
      "frequent mentions. Answer 1, while comprehensive, includes a lot of detailed information about\n",
      "various figures in different sectors of entertainment, which, while informative, does not directly\n"
     ]
    }
   ],
   "source": [
    "print(example1[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e24a8c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E\n",
      "System Prompts\n",
      "E.1\n",
      "Element Instance Generation\n",
      "---Goal---\n",
      "Given a text document that is potentially relevant to this activity and a list of entity types, identify\n",
      "all entities of those types from the text and all relationships among the identified entities.\n",
      "---Steps---\n",
      "1.\n",
      "Identify all entities.\n",
      "For each identified entity, extract the following information:\n",
      "- entity name:\n",
      "Name of the entity, capitalized\n",
      "- entity type:\n",
      "One of the following types:\n",
      "[{entity types}]\n",
      "- entity description:\n",
      "Comprehensive description of the entity’s attributes and activities\n",
      "Format each entity as (\"entity\"{tuple delimiter}<entity name>{tuple delimiter}<entity type>{tuple\n",
      "delimiter}<entity description>\n",
      "2.\n",
      "From the entities identified in step 1, identify all pairs of (source entity, target entity) that\n",
      "are *clearly related* to each other\n",
      "For each pair of related entities, extract the following information:\n",
      "- source entity:\n",
      "name of the source entity, as identified in step 1\n",
      "- target entity:\n",
      "name of the target entity, as identified in step 1\n",
      "- relationship description:\n"
     ]
    }
   ],
   "source": [
    "print(example2[\"chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eeb2b402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plore the effects of varying the context window size for our combinations of datasets, questions, and\n",
      "metrics. In particular, our goal was to determine the optimum context size for our baseline condition\n",
      "(SS) and then use this uniformly for all query-time LLM use. To that end, we tested four context\n",
      "window sizes: 8k, 16k, 32k and 64k. Surprisingly, the smallest context window size tested (8k)\n",
      "was universally better for all comparisons on comprehensiveness (average win rate of 58.1%), while\n",
      "performing comparably with larger context sizes on diversity (average win rate = 52.4%), and em-\n",
      "powerment (average win rate = 51.3%). Given our preference for more comprehensive and diverse\n",
      "answers, we therefore used a fixed context window size of 8k tokens for the final evaluation.\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(example3[\"chunk\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
