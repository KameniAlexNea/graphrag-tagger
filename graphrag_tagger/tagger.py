# topic_lib.py

import os
import json
from typing import List
import ktrain
from ktrain.text import get_topic_model
from langchain.schema import Document
from langchain.chat_models import ChatOllama  # or ChatOpenAI if preferred
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import PyPDFLoader

# -------------------------------------------
# Part 1: LDA-Based Topic Extraction using ktrain
# -------------------------------------------
class TopicExtractor:
    def __init__(self, n_features: int = 512, min_df: int = 2, max_df: float = 0.95, threshold: float = 0.25):
        """
        Initialize the TopicExtractor with ktrain parameters.
        """
        self.n_features = n_features
        self.min_df = min_df
        self.max_df = max_df
        self.threshold = threshold
        self.tm = None

    def fit(self, texts: List[str]):
        """
        Build the topic model using the provided texts.
        """
        self.tm = get_topic_model(texts, n_features=self.n_features, min_df=self.min_df, max_df=self.max_df)
        self.tm.build(texts, threshold=self.threshold)
        return self

    def get_topics(self) -> List[str]:
        """
        Retrieve the topics generated by the LDA model.
        """
        if self.tm is None:
            raise ValueError("Topic model not built. Call fit() first.")
        topics = self.tm.get_topics()
        return topics

    def filter_texts(self, texts: List[str]) -> List[str]:
        """
        Optionally filter texts using the topic model.
        """
        if self.tm is None:
            raise ValueError("Topic model not built. Call fit() first.")
        return self.tm.filter(texts)


# -------------------------------------------
# Part 2: LLM-Driven Topic Cleaning
# -------------------------------------------
class TopicCleaner:
    def __init__(self, model: str = "phi4", temperature: float = 0.2):
        """
        Initialize the TopicCleaner with an LLM. (Here we use ChatOllama.)
        """
        self.llm = ChatOllama(model=model, temperature=temperature)
        self.prompt_template = ChatPromptTemplate.from_template(
            """
You are an expert in summarizing and clarifying topics. Your task is to transform a list of messy topics generated by an LDA model into clear, concise topic labels.

Here is the list of messy topics:
<topics>
{topics}
</topics>

Return your answer as a JSON list of strings.
            """.strip()
        )

    def clean_topics(self, messy_topics: List[str]) -> List[str]:
        """
        Clean up the messy topics and return a list of clear topic labels.
        """
        topics_str = "\n".join(messy_topics)
        prompt = self.prompt_template.format(topics=topics_str)
        response = self.llm.invoke(prompt)
        try:
            # Clean and parse the JSON output.
            text = response.content.replace("```json", "").replace("```", "").strip()
            clean_topics = json.loads(text)
        except Exception as e:
            clean_topics = []
            print("Error parsing JSON:", e)
        return clean_topics


# -------------------------------------------
# Part 3: LLM-Driven Topic Classification for Document Chunks
# -------------------------------------------
class TopicClassifier:
    def __init__(self, model: str = "phi4", temperature: float = 0.2):
        """
        Initialize the TopicClassifier with an LLM.
        """
        self.llm = ChatOllama(model=model, temperature=temperature)
        self.prompt_template = ChatPromptTemplate.from_template(
            """
You are an expert content classifier.
Your task is to analyze the following text excerpt and select up to 3 topics from the provided candidate topics that best describe the content.

Text Excerpt:
<text>
{text}
</text>

Candidate Topics:
<topics>
{topics}
</topics>

Return your answer as a JSON array of strings (using only the provided candidate topics).
            """.strip()
        )

    def classify_chunk(self, chunk_text: str, candidate_topics: List[str]) -> List[str]:
        """
        For a given text chunk, select up to three topics from the candidate list.
        """
        topics_str = ", ".join(candidate_topics)
        formatted_prompt = self.prompt_template.format(
            text=chunk_text,
            topics=topics_str
        )
        response = self.llm.invoke(formatted_prompt)
        try:
            selected_topics = json.loads(response.content)
        except Exception as e:
            selected_topics = []
            print("Error parsing JSON:", e)
        return selected_topics


# -------------------------------------------
# Helper: Document Loader (example for PDFs)
# -------------------------------------------
def load_dataset(pdf_dir: str) -> List[Document]:
    """
    Load all PDF documents from the given directory.
    """
    documents = []
    for filename in os.listdir(pdf_dir):
        if filename.lower().endswith(".pdf"):
            loader = PyPDFLoader(os.path.join(pdf_dir, filename))
            docs = loader.load()
            documents.extend(docs)
    return documents


# -------------------------------------------
# Example Usage (if run as a script)
# -------------------------------------------
if __name__ == "__main__":
    # 1. Load your dataset (replace with your own directory path)
    dataset_path = "path/to/your/pdf_directory"
    all_documents = load_dataset(dataset_path)
    print("Loaded documents:", len(all_documents))

    # 2. Extract texts from documents for LDA topic modeling
    texts = [doc.page_content for doc in all_documents]
    print("Number of texts:", len(texts))

    # 3. Build the topic model using LDA (ktrain)
    extractor = TopicExtractor(n_features=512, min_df=2, max_df=0.95, threshold=0.25)
    extractor.fit(texts)
    messy_topics = extractor.get_topics()
    print("Messy topics from LDA:")
    for t in messy_topics:
        print("-", t)

    # 4. Clean the topics using an LLM
    cleaner = TopicCleaner(model="phi4", temperature=0.2)
    clean_topics = cleaner.clean_topics(messy_topics)
    print("Clean Topics:")
    for t in clean_topics:
        print("-", t)

    # 5. Classify a random document chunk using the clean topics as candidate topics
    classifier = TopicClassifier(model="phi4", temperature=0.2)
    example_chunk = all_documents[0].page_content
    selected_topics = classifier.classify_chunk(example_chunk, clean_topics)
    print("Selected Topics for first document chunk:")
    print(selected_topics)
